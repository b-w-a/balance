---
title: 'Stats: Final Project'
author: "Bryan Wilcox + Tyler Reny"
date: "February 20, 2016"
output: pdf_document
---

```{r setup, warning=FALSE, message=FALSE, echo=FALSE, results='hide'}
knitr::opts_chunk$set(cache=TRUE)
library(caret)
library(foreign)
library(dplyr)
library(date)
library(car)
library(knitr)
library(MASS)
library(e1071)
library(glmnet)
library(ROCR)
library(KRLS)
source("krlogit_trunc_2.R")


df <- read.dta("CS_mmn_recoded2.dta", convert.dates = F)

drop <- which(df$us_citizen == 0)
df <- df[-drop,]
df$us_citizen %>% table()

df$state <- tolower(df$state)

#drop DC
drop <- which(df$state == "dc")
df <- df[-drop,]
table(df$state == "dc")

drop <- which(df$month < 9)
df <- df[-drop,]

table(df$state=="id")
table(df$state=="ms")
table(df$state=="mt")
table(df$state=="ri")
table(df$state=="vt")
table(df$state=="wy")
drop <- which(df$state=="id" | df$state=="ms" | df$state=="mt" |df$state=="ri" |df$state=="vt" | df$state=="wy")
df <- df[-drop,]

df$BGVEP <- ifelse(df$state == "ar" | df$state=="de" | df$state=="fl" |
                     df$state == "il" | df$state=="ia" | df$state=="ky"|
                     df$state == "la" | df$state=="me" | df$state=="mi" |
                     df$state == "mn" | df$state=="mo" | df$state=="mt" |
                     df$state == "nv" | df$state=="nh" | df$state=="nm" |
                     df$state == "oh" | df$state=="or" | df$state=="pa" |
                     df$state == "tn" | df$state=="wa" | df$state=="wi" , 1, 0)

df$BGCNN <- ifelse(df$state == "az" | df$state=="ar" | df$state=="de" |
                     df$state == "fl" | df$state=="il" | df$state=="ia"|
                     df$state == "la" | df$state=="me" | df$state=="mi" |
                     df$state == "mo" | df$state=="nv" | df$state=="nh" |
                     df$state == "nm" | df$state=="oh" | df$state=="or" |
                     df$state == "pa" | df$state=="tn" | df$state=="wa" |
                     df$state == "wv" | df$state=="wi" , 1, 0)

df$BATTLEGROUND <- df$BGVEP
df <- df[-c(which(df$BATTLEGROUND==1)),]

sd <- factor(df$state)
mm <- model.matrix(~factor(sd) - 1)
colnames(mm) <- c("sd_1","sd_2","sd_3","sd_4","sd_5",
                  "sd_6","sd_7","sd_8","sd_9","sd_10",
                  "sd_11","sd_12","sd_13","sd_14","sd_15",
                  "sd_16","sd_17","sd_18","sd_19","sd_20")
mm <- mm[,-3] #exclude california
df <- cbind(df, mm)

df$statecd <- paste(toupper(df$state),df$cu01,sep="")

df$comphouse <- ifelse(df$statecd=="AR4"	|
df$statecd=="AR4"	| df$statecd=="CA15"	| df$statecd=="CA20"	| df$statecd=="CA27"	|
df$statecd=="CA36"	| df$statecd=="CA49"	| df$statecd=="CT2"	| df$statecd=="CT5"	|
df$statecd=="FL12"	| df$statecd=="FL22"	| df$statecd=="FL3"	| df$statecd=="FL8"	|
df$statecd=="IL10"	| df$statecd=="IL17"	| df$statecd=="IN8"	| df$statecd=="KS3"	|
df$statecd=="KY1"	| df$statecd=="KY3"	| df$statecd=="KY6"	| df$statecd=="MI8"	|
df$statecd=="MN6"	| df$statecd=="MO6"	| df$statecd=="MS4"	| df$statecd=="MT1"	|
df$statecd=="NC11"	| df$statecd=="NC8"	| df$statecd=="NH2"	| df$statecd=="NJ12"	|
df$statecd=="NJ7"	| df$statecd=="NM1"	| df$statecd=="NV1"	| df$statecd=="NY2"	|
df$statecd=="OH12"	| df$statecd=="OK2"	| df$statecd=="PA10"	| df$statecd=="PA13"	|
df$statecd=="PA4"	| df$statecd=="UT2"	| df$statecd=="VA2"	| df$statecd=="WA1"	|
df$statecd=="WA2"	| df$statecd=="WA5"	| df$statecd=="WV2" ,1,0)

x <- mdy.date(df$month,df$day,rep(2000,nrow(df)))
tmp <- as.numeric(x)
df$interviewweekdummy <- as.integer((tmp-14854)/7)
wd <- model.matrix(~factor(df$interviewweekdummy)-1)
colnames(wd) <- c("wd_1","wd_2","wd_3","wd_4","wd_5",
                  "wd_6","wd_7","wd_8","wd_9","wd_10")
wd <- wd[,-1] #drop wd1 as base category
df <- cbind(df,wd)

df$ms4_bushmargin_grp <- df$ms4_pro_bush-df$ms4_pro_gore_grp

df$age2 <- df$age*df$age

df$abspid5 <- abs(df$pid5) #fold partisanship over on itself

df$absideology <- abs(df$ideology) #fold ideology over on itself

df$media_zone_text <- paste(df$mmn,df$state, sep="")
df$media_zone <- as.factor(df$media_zone_text)

###OTHER VARIABLES OF INTEREST

df$watch_tv <- df$ce01
df$watch_tv <- recode(df$watch_tv, "998:999=NA") #7 is highest
df$newspaper <- df$ce13
df$newspaper <- recode(df$newspaper, "998:999=NA") #7 is highest
df$talk_radio <- df$ce18
df$talk_radio <- recode(df$talk_radio, "998:999=NA") #7 is highest
df$married <- df$cw08
df$married <- recode(df$married, "2:6=0;998:999=NA") #7 is highest
df$health_insurance <- df$cw20
df$health_insurance <- recode(df$health_insurance, "2=0;998:999=NA") #7 is highest

df$ad_dummy <- ifelse(df$ms4_presadgrp > 0,1,0)

df_clean <- df %>% dplyr::select(ad_dummy,ideology,pid5,attend_church,union,income,income_dkna,employed,education,hispanic,white,female,age,watch_tv,newspaper,talk_radio,married,health_insurance) %>% na.omit()

set.seed(1234)
trainIndex <- createDataPartition(df_clean$ad_dummy, p = 0.75, list = F, times = 1)

df_train <- df_clean[ trainIndex,]
df_test  <- df_clean[-trainIndex,]
```

#Logit 

```{r}
model <- ad_dummy ~ ideology + pid5 + attend_church + union + income + income_dkna + employed + education + hispanic + white + female + age + watch_tv + newspaper + talk_radio + as.numeric(married) + as.numeric(health_insurance)

glm.out <- glm(model, family = binomial(link="logit"), data = df_train)

#out of sample
yhat <- predict(glm.out, newdata=df_test, type="response")
yhat <- ifelse(yhat > .5, 1, 0)
out <- prop.table(table(predicted=yhat, truth=df_test$ad_dummy))
sum(diag(out))
stargazer(out) #overfitting
```

#LDA

```{r}
lda.test <- lda(model, data = df_train, CV=F)

#out of sample
yhat.lda.test <- predict(object = lda.test, newdata = df_test)$class
lda.table <- round(prop.table(table(truth=df_test$ad_dummy, predicted=yhat.lda.test)),3)
colnames(lda.table) <- c("non-treatment","treatment")
rownames(lda.table) <- c("non-treatment","treatment")
kable(lda.table, format = "latex")
sum(diag(lda.table))
```

#QDA

For qda to work correctly - you have to make sure the `married` and `health_insurance` are not factor variables. 
```{r}
qda.fit <- qda(model,data = df_train, CV=FALSE)

#out of sample
yhat.qda.test <- predict(qda.fit,newdata=df_test)$class
qda.table <- round(prop.table(table(truth=df_test$ad_dummy, predicted=yhat.qda.test)),3)
colnames(qda.table) <- c("non-treatment","treatment")
rownames(qda.table) <- c("non-treatment","treatment")
kable(qda.table, format = "latex")
```

#NBC

```{r}
nbc.fit <- naiveBayes(model, data = df_train)


#out of sample
nbc.test.p <- predict(nbc.fit, newdata = df_test,type = "raw")
yhat.nbc.test <- apply(nbc.test.p,1,function(x) which(x==max(x)))

prop.table(table(df_test$ad_dummy, yhat.nbc.test))
nbc.table <- round(prop.table(table(df_test$ad_dummy, yhat.nbc.test)),3)

colnames(nbc.table) <- c("non-treatment","treatment")
rownames(nbc.table) <- c("non-treatment","treatment")
kable(nbc.table, format = "latex")
```

#Lasso Logit

```{r}
df_train$married <- df_train$married %>% as.numeric
df_train$health_insurance <- df_train$health_insurance %>% as.numeric

logit.out <- cv.glmnet(y=as.matrix(df_train[,1]), x=as.matrix(df_train[,2:18]), family="binomial")
lb <- logit.out$lambda.min
lasso.out <- glmnet(y=as.matrix(df_train[,1]), x=as.matrix(df_train[,2:18]), family="binomial", lambda = lb)
coef(lasso.out)  
```

#SVM

```{r}
df_train$married <- df_train$married %>% as.numeric
df_train$health_insurance <- df_train$health_insurance %>% as.numeric
df_test$married <- df_test$married %>% as.numeric
df_test$health_insurance <- df_test$health_insurance %>% as.numeric

set1 <- data.frame(y=(df_train$ad_dummy),x=df_train[,c(2:18)])
str(set1)
set2 <- data.frame(y=as.factor(df_test$ad_dummy),x=df_test[,c(2:18)])

fit1 <- svm(y ~ ., data = set1, kernel="radial", cost = 5)

pred.orig <- predict(fit1,set2)
pred.orig <- ifelse(pred.orig>.5,1,0)
sum(diag(prop.table(table(predicted=pred.orig,truth=set2$y))))
```

# KRLS and KRLogit

```{r}
Xtrain=df_train[1:1000,c(2:18)] %>% as.matrix
ytrain=df_train[1:1000,1] %>% as.matrix
Xtest=df_train[1001:2000,c(2:18)] %>% as.matrix
ytest=df_train[1001:2000,1] %>% as.matrix

krls.out <- krls(X = Xtrain, y = ytrain, lambda=2000)
names(krls.out)

krls.test <- predict(krls.out, newdata = Xtest)

#out of sample
krls.test <- predict(krls.out, newdata = Xtest)
krls.y.fit <- ifelse(krls.test$fit > .5, 1,0)
prop.table(table(true = ytest, predicted = krls.y.fit ))


####KRLS LOGIT
#deriv if I want pointwise marginal effects -- can be used to get histograms
#lambdarange <- doesn't have a fast search, so look at output and 
#lambda folds = 5
#
krlogit.out <- krlogit(X=Xtrain,y=ytrain, deriv = T, epsilon = .01, lambdarange = c(5, 10,100, 500))

pwme <- krlogit.out$derivmat
histogram(as.formula(paste("~",paste(colnames(pwme),collapse="+"))), data=as.data.frame(pwme), breaks=)

krlogit.test <- predict(krlogit.out, newdata = Xtest)
yhat <- ifelse(krlogit.test$fit > .5, 1, 0)
cor(yhat,ytest)
prop.table(table(true=ytest, pred=yhat))

names(krlogit.out)
krlogit.out$avgderiv

#Do ridge, do lasso, dial lambda down , do something that will give us R squared.
#p.ROC #sig test
#can ask SVM for a probability and use for RCs
#ends up more often at extremes of one x when treated.
```

#ROC CURVES

```{r}
prob <- predict(glm.out, newdata=df_test, type="response")
prob <- ifelse(prob > .5, 1, 0)
pred <- prediction(as.numeric(prob), as.matrix(df_test$ad_dummy))
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
auc <- performance(pred, measure = "auc")
auc.glm <- auc@y.values[[1]]

roc.data.glm <- data.frame(fpr=unlist(perf@x.values),
                       tpr=unlist(perf@y.values),
                       model="GLM")
ggplot(roc.data.glm, aes(x=fpr, ymin=0, ymax=tpr)) +
    geom_ribbon(alpha=0.2) +
    geom_line(aes(y=tpr)) +
    annotate("text", x=.1, y=.8, label=paste("Logit", round(auc.glm,3), sep=" ")) +
    geom_abline(intercept=0, slope=1, color="red", linetype=2) +
  theme_minimal()

#LDA
prob <- predict(object = lda.test, newdata = df_test)$class
pred <- prediction(as.numeric(prob), as.matrix(df_test$ad_dummy))
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
auc <- performance(pred, measure = "auc")
auc.lda <- auc@y.values[[1]]

roc.data.lda <- data.frame(fpr=unlist(perf@x.values),
                       tpr=unlist(perf@y.values),
                       model="LDA")
ggplot(roc.data.lda, aes(x=fpr, ymin=0, ymax=tpr)) +
    geom_ribbon(alpha=0.2) +
    geom_line(aes(y=tpr)) +
    annotate("text", x=.1, y=.8, label=paste("LDA", round(auc.lda,3), sep=" ")) +
    geom_abline(intercept=0, slope=1, color="red", linetype=2) +
  theme_minimal()

#Naive Bayes

pred <- predict(nbc.fit, newdata = df_train,type = "raw")
pred <- prediction(as.numeric(prob), as.matrix(df_test$ad_dummy))
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
auc <- performance(pred, measure = "auc")
auc.nb <- auc@y.values[[1]]

roc.data.nb <- data.frame(fpr=unlist(perf@x.values),
                       tpr=unlist(perf@y.values),
                       model="Naive Bayes")
ggplot(roc.data.nb, aes(x=fpr, ymin=0, ymax=tpr)) +
    geom_ribbon(alpha=0.2) +
    geom_line(aes(y=tpr)) +
    annotate("text", x=.1, y=.8, label=paste("Naive \nBayes", round(auc.nb,3), sep=" ")) +
    geom_abline(intercept=0, slope=1, color="red", linetype=2) +
  theme_minimal()

#QDA

prob <- predict(qda.fit,newdata=df_test)$class
pred <- prediction(as.numeric(prob), as.matrix(df_test$ad_dummy))
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
auc <- performance(pred, measure = "auc")
auc.qda <- auc@y.values[[1]]

roc.data.qda <- data.frame(fpr=unlist(perf@x.values),
                       tpr=unlist(perf@y.values),
                       model="QDA")
ggplot(roc.data.qda, aes(x=fpr, ymin=0, ymax=tpr)) +
    geom_ribbon(alpha=0.2) +
    geom_line(aes(y=tpr)) +
    annotate("text", x=.1, y=.8, label=paste("QDA", round(auc.qda,3), sep=" ")) +
    geom_abline(intercept=0, slope=1, color="red", linetype=2) + theme_minimal()

#SVM

prob <- predict(fit1,set2)
pred <- prediction(as.numeric(prob), as.matrix(df_test$ad_dummy))
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
auc <- performance(pred, measure = "auc")
auc.svm <- auc@y.values[[1]]

roc.data.svm <- data.frame(fpr=unlist(perf@x.values),
                       tpr=unlist(perf@y.values),
                       model="SVM")
ggplot(roc.data.svm, aes(x=fpr, ymin=0, ymax=tpr)) +
    geom_ribbon(alpha=0.2) +
    geom_line(aes(y=tpr)) +
    annotate("text", x=.1, y=.8, label=paste("SVM", round(auc.svm,3), sep=" ")) +
    geom_abline(intercept=0, slope=1, color="red", linetype=2) + theme_minimal()

#KRLS

#out of sample
krls.test <- predict(krls.out, newdata = Xtest)
prob <- krls.test$fit
pred <- prediction(prob, ytest)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
auc <- performance(pred, measure = "auc")
auc.krls <- auc@y.values[[1]]

roc.data.krls <- data.frame(fpr=unlist(perf@x.values),
                       tpr=unlist(perf@y.values),
                       model="KRLS")
ggplot(roc.data.krls, aes(x=fpr, ymin=0, ymax=tpr)) +
    geom_ribbon(alpha=0.2) +
    geom_line(aes(y=tpr)) +
    annotate("text", x=.1, y=.8, label=paste("KRLS", round(auc.krls,3), sep=" ")) +
    geom_abline(intercept=0, slope=1, color="red", linetype=2) + theme_minimal()

#KRLOGIT
prob <- krlogit.test$fit
pred <- prediction(prob, ytest)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
auc <- performance(pred, measure = "auc")
auc.krlogit <- auc@y.values[[1]]

roc.data.krlogit <- data.frame(fpr=unlist(perf@x.values),
                       tpr=unlist(perf@y.values),
                       model="KR Logit")
ggplot(roc.data.krlogit, aes(x=fpr, ymin=0, ymax=tpr)) +
    geom_ribbon(alpha=0.2) +
    geom_line(aes(y=tpr)) +
    annotate("text", x=.1, y=.8, label=paste("KR Logit", round(auc.krlogit,3), sep=" ")) +
    geom_abline(intercept=0, slope=1, color="red", linetype=2) + theme_minimal()

####
#Combine all
####

fulldata <- rbind(roc.data.glm,
                  roc.data.lda,
                  roc.data.qda,
                  roc.data.nb,
                  roc.data.svm,
                  roc.data.krls,
                  roc.data.krlogit)

#### PLOT 1
# GLM, LDA, QDA, NB

g.out <- ggplot(fulldata[fulldata$model=="GLM" |fulldata$model=="LDA"| fulldata$model=="QDA" | fulldata$model=="Naive Bayes",], aes(x=fpr,  y=tpr, group=model, color=model)) +
  geom_line() +
  theme(legend.position="none") +
  labs(x="", y="") +
  geom_abline(intercept=0, slope=1, color="grey", linetype=2) + 
  theme_minimal() +
  labs(y="sensitivity", x="specificity") +
  annotate("text", x=rep(.15,5), 
           y=c(1,.95, .9, .85, .8), 
           label=c("Area Under Curve",
                   paste("QDA", round(auc.qda,3), sep=" "),
                   paste("GLM", round(auc.glm,3), sep=" "),
                   paste("LDA", round(auc.lda,3), sep=" "),
                   paste("Naive Bayes", round(auc.nb,3), sep=" ")))

ggsave(g.out, width=6, height=5,file= "roc1.png")

g.out2<- ggplot(fulldata[fulldata$model=="SVM" |fulldata$model=="KRLS"| fulldata$model=="KR Logit",], aes(x=fpr,  y=tpr, group =model, color=model)) +
  geom_line() +
  theme(legend.position="none") +
  labs(x="", y="") +
  geom_abline(intercept=0, slope=1, color="grey", linetype=2) + 
  theme_minimal() +
  annotate("text", x=rep(.15,4), 
           y=c(1,.95, .9, .85), 
           label=c("Area Under Curve",
                   paste("SVM", round(auc.svm,3), sep=" "),
                   paste("KRLS", round(auc.krls,3), sep=" "),
                   paste("KR Logit", round(auc.krlogit,3), sep=" ")))
ggsave(g.out2, width=6, height=5,file= "roc2.png")

```


